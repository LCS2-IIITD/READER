# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01-gpt2-with-value-head.ipynb (unless otherwise specified).

__all__ = ['CausalLMOutputWithCrossAttentions', 'ValueHead', 'GPT2HeadWithValueModel', 'respond_to_batch']

# Cell
device = "cuda:1"
from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Model, GPT2PreTrainedModel
from transformers import top_k_top_p_filtering
from transformers.modeling_outputs import ModelOutput
from torch import nn
from torch.nn import Identity
from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
import torch.nn.functional as F
import torch
from dataclasses import dataclass
from typing import Optional, Tuple

# Cell
@dataclass
class CausalLMOutputWithCrossAttentions(ModelOutput):
    dac_pred: Optional[torch.FloatTensor] = None
    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None
    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None
    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None
    value: Optional[torch.FloatTensor] = None
    # labels: Optional[torch.LongTensor] = None


# Cell

class ValueHead(nn.Module):
    """The ValueHead class implements a head for GPT2 that returns a scalar for each output token."""
    def __init__(self, config):
        super().__init__()
        self.detach_head = False
        self.summary_type = config.summary_type if hasattr(config, "summary_type") else "last"
        if self.summary_type == "attn":
            raise NotImplementedError

        self.summary = Identity()
        if hasattr(config, "summary_use_proj") and config.summary_use_proj:
            if hasattr(config, "summary_proj_to_labels") and config.summary_proj_to_labels and config.num_labels > 0:
                num_classes = config.num_labels
            else:
                num_classes = config.hidden_size
            self.summary = nn.Linear(config.hidden_size, num_classes)

        self.activation = Identity()
        if hasattr(config, "summary_activation") and config.summary_activation == "tanh":
            self.activation = nn.Tanh()

        self.first_dropout = Identity()
        if hasattr(config, "summary_first_dropout") and config.summary_first_dropout > 0:
            self.first_dropout = nn.Dropout(config.summary_first_dropout)

        self.last_dropout = Identity()
        if hasattr(config, "summary_last_dropout") and config.summary_last_dropout > 0:
            self.last_dropout = nn.Dropout(config.summary_last_dropout)

        self.flatten = nn.Flatten()

    def forward(self, hidden_states, cls_index=None):
        if self.detach_head:
            output = hidden_states.detach()
        else:
            output = hidden_states
        output = self.first_dropout(output)
        output = self.summary(output)
        output = self.activation(output)
        output = self.last_dropout(output)

        return output

    
class DacHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.l0 = nn.Linear(768,768)
        self.l1 = nn.Linear(768,500)
        self.l2 = nn.Linear(500,12)
        self.dropout1 = nn.Dropout(0.5)
        self.dropout2 = nn.Dropout(0.5)
        
    def forward(self, hidden_states, cls_index=None):
        # print("hidden size: ")
        # print(hidden_states.size())
        pooled_output = torch.mean(hidden_states, dim = 1)
        # print(pooled_output.size())
        x0 = self.l0(pooled_output)
        
        x0 = F.relu(x0)
        x1 = self.dropout2(x0)
        x1 = F.relu(x1)
        x2 = self.l1(x1)
        x2 = F.relu(x2)
        x3 = self.dropout1(x2)
        x3 = F.relu(x3)
        x4 = self.l2(x3)
        return x4
# Cell

class GPT2HeadWithValueModel(GPT2PreTrainedModel):
    """The GPT2HeadWithValueModel class implements a GPT2 language model with a secondary, scalar head."""
    def __init__(self, config):
        super().__init__(config)
        config.num_labels = 1
        self.transformer = GPT2Model(config)
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        # self.lm_head = self.transformer.lm_head
        self.v_head = ValueHead(config)
        self.dac_head = DacHead(config)

        self.init_weights()

    def get_output_embeddings(self):
        return self.lm_head

    def detach_value_head(self):
        self.v_head.detach_head = True

    def forward(
        self,
        input_ids=None,
        labels: Optional[torch.LongTensor] = None,
        past_key_values=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        mc_token_ids=None,
        lm_labels=None,
        mc_labels=None,
        return_dict=False,
        output_attentions=False,
        output_hidden_states=False,
    
    ):
        loss=None
        transformer_outputs = self.transformer(
            input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds
        )

        hidden_states = transformer_outputs[0]

        lm_logits = self.lm_head(hidden_states)
        
        value = self.v_head(hidden_states).squeeze(-1)
        
        dac_pred = self.dac_head(hidden_states)

        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = lm_logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = CrossEntropyLoss(ignore_index = 50259)
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))


        return CausalLMOutputWithCrossAttentions(
            dac_pred = dac_pred,
            loss=loss,
            logits=lm_logits,
            past_key_values=transformer_outputs.past_key_values,
            hidden_states=transformer_outputs.hidden_states,
            attentions=transformer_outputs.attentions,
            cross_attentions=transformer_outputs.cross_attentions,
            value=value,
        )
        return outputs

# Cell

def respond_to_batch(model, queries, masks, repetition_penalty):
    """Sample text from language model."""
    txt_len=20
    top_k=6
    top_p=0.9
    input_ids = queries
    attn_masks = masks
    for i in range(txt_len):
        
        # Get Logits
        outputs = model(input_ids = input_ids)
        next_token_logits = outputs.logits[:, -1, :]
        next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)
        
        if repetition_penalty != 1.0:
                for i in range(input_ids.size()[0]):
                    for previous_tokens in set(input_ids[i].tolist()):
                        next_token_logits[i, previous_tokens] = -1000
                        
        probs = F.softmax(next_token_logits)
        next_token = torch.multinomial(probs, num_samples=1).squeeze(1)
        
        input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)
       
    return input_ids[:, -txt_len:]

